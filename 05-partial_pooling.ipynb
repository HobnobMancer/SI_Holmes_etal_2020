{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Information: Holmes *et al.* 2017\n",
    "\n",
    "# 5. Partial pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lpritc/Development/GitHub/Supplementary_Info/SI_Holmes_etal_2016/venv-SI_Holmes_2016/lib/python3.5/site-packages/Cython/Distutils/old_build_ext.py:30: UserWarning: Cython.Distutils.old_build_ext does not properly handle dependencies and is deprecated.\n",
      "  \"Cython.Distutils.old_build_ext does not properly handle dependencies \"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystan\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import tools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "We have, so far, restricted ourselves to pooled or unpooled linear fits. This has had the effect of either: maximising the effect of *bias* (for the pooled model) - where a single model can't possibly fit every circumstance perfectly; or maximising the effect of *variance* (for the unpooled model) - where we run the risk of overfitting our model to occasionally unreliably outlying data by having completely independent linear fits for each group (*probe ID*)\n",
    "\n",
    "An alternative to this is to assume that the parameters of the model for each *probe ID* are drawn from some parent distribution, in a *hierarchical model*. This is in effect equivalent to fitting a different linear model to each *probe ID*, but specifying that the range of parameters for each model is constrained to a certain distribution of values.\n",
    "\n",
    "**NOTE:** it is entirely possible that our choice of distribution, and choice of grouping strategy, are inappropriate for the data.\n",
    "\n",
    "We therefore construct the following model of the experiment:\n",
    "\n",
    "$$y_i = \\alpha_{j[i]} + \\beta_{j[i]} x_i + \\epsilon_i$$\n",
    "$$\\alpha_{j[i]} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)$$\n",
    "$$\\beta_{j[i]} \\sim N(\\mu_{\\beta}, \\sigma_{\\beta}^2)$$\n",
    "$$\\sigma_{\\alpha} \\sim U(0, 100)$$\n",
    "$$\\sigma_{\\beta} \\sim U(0, 100)$$\n",
    "\n",
    "* $y_i$: measured intensity output on the array for probe $i$ (specific to each replicate)\n",
    "* $x_i$: measured intensity input on the array for probe $i$ (specific to each replicate)\n",
    "* $\\alpha_{j[i]}$: the linear intercept, this is a constant 'offset' for all *output* measurements relative to *input* measurements, but it differs for each *probe ID*; this is drawn from a Normal distribution $\\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)$\n",
    "* $\\mu_{\\alpha}$: the mean offset for all *probe ID*s\n",
    "* $\\sigma_{\\alpha}$: the standard deviation of the offset for all *probe ID*s - sampled from a Uniform distribution $U(0, 100)$\n",
    "* $\\beta_{j[i]}$: the linear slope, this is the relative change in measured intensity between *input* and *output* intensities *at the probe ID level* - it differs for each *probe ID*; this is drawn from a Normal distribution $\\sim N(\\mu_{\\beta}, \\sigma_{\\beta}^2)$\n",
    "* $\\mu_{\\beta}$: the mean slope for all *probe ID*s\n",
    "* $\\sigma_{\\beta}$: the standard deviation of the slope for all *probe ID*s - sampled from a Uniform distribution $U(0, 100)$\n",
    "* $\\epsilon_i$: error in the model prediction for probe $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stan model construction and fit\n",
    "\n",
    "We need to define `data`, `parameters` and our `model` for `Stan`.\n",
    "\n",
    "In the `data` block, we have:\n",
    "\n",
    "* `N`: `int`, the number of data points)\n",
    "* `J`: `int`, the number of unique probe IDs (`J` < `N`)\n",
    "* `probe`: `int[N]`, an index list of probe identities - one index representing six probe measurements (i.e. three control, three treatment) - there are `J` probes\n",
    "* `x`: `vector[N]`, the input log(intensity) values\n",
    "* `y`: `vector[N]`, the output log(intensity) values\n",
    "\n",
    "In the `parameter` block, we have:\n",
    "\n",
    "* `a`: `real vector[J]`, representative input log(intensity)\n",
    "* `mu_a`: `real`, an unconstrained value to be fit that represents the mean offset for each probe ID\n",
    "* `b`: `real vector[J]`, effect on log(intensity) of passing through the experiment, specific to a probe ID\n",
    "* `mu_b`: `real`, an unconstrained value to be fit that represents the mean slope for each probe ID\n",
    "* `sigma`: `real<lower=0>`, the error in the prediction\n",
    "* `sigma_a`: `real<lower=0,upper=100>`, standard deviation of the offset per probe ID\n",
    "* `sigma_b`: `real<lower=0,upper=100>`, standard deviation of the slope per probe ID\n",
    "\n",
    "We also define a `transformed parameter`:\n",
    "\n",
    "* `y_hat[i] <- b[probe[i]] * x[i] + a[probe[i]]`: the linear relationship describing $\\hat{y}$, our estimate of experimental output intensity, which is subject to variance `sigma`.\n",
    "\n",
    "We define the model as:\n",
    "\n",
    "$$\\sigma_{\\alpha} \\sim U(0, 100)$$\n",
    "$$\\alpha_{j[i]} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)$$\n",
    "$$\\sigma_{\\beta} \\sim U(0, 100)$$\n",
    "$$\\beta_{j[i]} \\sim N(\\mu_{\\beta}, \\sigma_{\\beta}^2)$$\n",
    "$$y \\sim N(\\hat{y}, \\sigma^2)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load clean, normalised, indexed data\n",
    "# data = pd.read_csv(\"output/normalised_array_data.tab\", sep=\"\\t\")  # full dataset\n",
    "data = pd.read_csv(\"output/reduced_normalised_indexed_array_data.tab\", sep=\"\\t\")  # reduced dataset for test\n",
    "\n",
    "# useful values\n",
    "probe_ids = data['probe'].unique()\n",
    "nprobes = len(probe_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define unpooled stan model\n",
    "partial_model = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0> J;\n",
    "  int<lower=1, upper=J> probe[N];\n",
    "  vector[N] x;\n",
    "  vector[N] y;\n",
    "}\n",
    "parameters {\n",
    "  vector[J] a;\n",
    "  vector[J] b;\n",
    "  real mu_a;\n",
    "  real mu_b;\n",
    "  real<lower=0> sigma;\n",
    "  real<lower=0,upper=100> sigma_a;\n",
    "  real<lower=0,upper=100> sigma_b;\n",
    "}\n",
    "transformed parameters{\n",
    "  vector[N] y_hat;\n",
    "\n",
    "  for (i in 1:N)\n",
    "    y_hat[i] = a[probe[i]] + b[probe[i]] * x[i];\n",
    "}\n",
    "model {\n",
    "  sigma_a ~ uniform(0, 100);\n",
    "  a ~ normal(mu_a, sigma_a);\n",
    "\n",
    "  sigma_b ~ uniform(0, 100);\n",
    "  b ~ normal(mu_b, sigma_b);\n",
    "\n",
    "y ~ normal(y_hat, sigma);\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# relate python variables to stan variables\n",
    "partial_data_dict = {'N': len(data),\n",
    "                     'J': nprobes,\n",
    "                     'probe': data['probe_index'] + 1,\n",
    "                     'x': data['input'],\n",
    "                     'y': data['output']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run stan fit\n",
    "partial_fit = pystan.stan(model_code=partial_model,\n",
    "                          data=partial_data_dict,\n",
    "                          iter=1000, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the fit\n",
    "\n",
    "Now there are (for the full dataset) now approximately 44000 parameters describing the slope and intercept for individual probe IDs. This is more than we can look at directly but we can continue to get an estimate of the range of values those slopes and look at the potential for outliers - specific probe IDs that look as though they may deviate quite far from the mean of all estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get fits to alpha and beta by probe ID\n",
    "alpha_estimates = pd.Series(partial_fit['a'].mean(0), index=probe_ids)\n",
    "alpha_se = pd.Series(partial_fit['a'].std(0), index=probe_ids)\n",
    "beta_estimates = pd.Series(partial_fit['b'].mean(0), index=probe_ids)\n",
    "beta_se = pd.Series(partial_fit['b'].std(0), index=probe_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the estimates\n",
    "alpha_estimates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the estimates\n",
    "beta_estimates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intercept $\\alpha_{j[i]}$\n",
    "\n",
    "Taking the intercept first, we can get an overview of the mean and standard error of $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot means distribution\n",
    "g = sns.boxplot(alpha_estimates)\n",
    "g.set_title(\"Distribution of mean intercept\")\n",
    "g.set_xlabel(\"mean(alpha)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot means distribution\n",
    "g = sns.boxplot(alpha_se)\n",
    "g.set_title(\"Distribution of mean intercept\")\n",
    "g.set_xlabel(\"se(alpha)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the unpooled model in particular, the distribution of intercepts is now rather tight, with a mean close to 25. However, the variance is also quite large, and we cannot say that our intercepts are different from zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# errors for intercept outliers\n",
    "tools.plot_threshold_errors(alpha_estimates, alpha_se, 26.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# errors for intercept outliers\n",
    "tools.plot_threshold_errors(alpha_estimates, alpha_se, 25, upper=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slopes $\\beta_{j[i]}$\n",
    "\n",
    "The variation of slopes $\\beta_{j[i]}$ by *probe ID* remain the most interesting estimates, as they are indicative of the way *output* intensity measurements depend on *input* intensity measurements, and could help identify genes that are advantageous or disadvantageous in our experiment.\n",
    "\n",
    "In our previous fit, we saw several outlying values of $\\beta_{j[i]}$, ranging quite far from unity, and that there was considerable error associated with the more extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot means distribution\n",
    "g = sns.boxplot(beta_estimates)\n",
    "g.set_title(\"Distribution of mean intercept\")\n",
    "g.set_xlabel(\"mean(beta)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot error distribution\n",
    "g = sns.boxplot(beta_se)\n",
    "g.set_title(\"Distribution of standard error\")\n",
    "g.set_xlabel(\"se(beta)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this partially-pooled model, we see that most values of $\\beta_{j[i]}$ lie close to 0.8, and the number of outliers (and the size of the parameter) is reduced. The error in the estimate for slope is also much smaller, and it appears that no slopes are negative, or have errors of their estimates that imply they could take a zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# errors for intercept outliers\n",
    "tools.plot_threshold_errors(beta_estimates, beta_se, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# errors for intercept outliers\n",
    "tools.plot_threshold_errors(beta_estimates, beta_se, 0.7, upper=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# errors for intercept outliers\n",
    "tools.plot_threshold_errors(beta_estimates, beta_se, 2, upper=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are a couple of obvious outliers, but in general most *probe ID*s have a fairly steady relationship between *input* and *output* measured intensity. There are noticeable differences in estimated slope, but the greatest uncertainty now seems to lie with *probe ID* with estimated slopes close to the median value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we still have not accounted for a number of other potential influences on model output, including the effect of *control* vs *treatment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
